{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "input_image = load_img(\"photo.jpg\")\n",
    "input_image_array = img_to_array(input_image)\n",
    "#print(input_image_array)\n",
    "# Get the dimensions of the input image\n",
    "rows, cols, channels = input_image_array.shape\n",
    "\n",
    "print(rows,cols,channels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS FOR TRAINING AND TESTING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_path = './data/'\n",
    "train_path = './train/'\n",
    "test_path = './test/'\n",
    "# Create the folders\n",
    "os.makedirs(train_path + 'vehicles', exist_ok=True)\n",
    "os.makedirs(train_path + 'non-vehicles', exist_ok=True)\n",
    "os.makedirs(test_path + 'vehicles', exist_ok=True)\n",
    "os.makedirs(test_path + 'non-vehicles', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Define the source and destination paths\n",
    "src_vehicles_path = base_path + 'vehicles/'\n",
    "src_non_vehicles_path = base_path + 'non-vehicles/'\n",
    "\n",
    "dest_train_vehicles_path = train_path + 'vehicles/'\n",
    "dest_train_non_vehicles_path = train_path + 'non-vehicles/'\n",
    "dest_test_vehicles_path = test_path + 'vehicles/'\n",
    "dest_test_non_vehicles_path = test_path + 'non-vehicles/'\n",
    "\n",
    "#split the data into 80% for training and 20% for testing\n",
    "split_ratio = 0.8\n",
    "\n",
    "# List all vehicle and non-vehicle images\n",
    "vehicle_images = os.listdir(src_vehicles_path)\n",
    "non_vehicle_images = os.listdir(src_non_vehicles_path)\n",
    "\n",
    "# Calculate the number of images for training and testing\n",
    "num_train_vehicles = int(len(vehicle_images) * split_ratio)\n",
    "num_train_non_vehicles = int(len(non_vehicle_images) * split_ratio)\n",
    "\n",
    "# Copy images to the training folders\n",
    "for img in vehicle_images[:num_train_vehicles]:\n",
    "    shutil.copy(src_vehicles_path + img, dest_train_vehicles_path + img)\n",
    "\n",
    "for img in non_vehicle_images[:num_train_non_vehicles]:\n",
    "    shutil.copy(src_non_vehicles_path + img, dest_train_non_vehicles_path + img)\n",
    "\n",
    "# Copy the remaining images to the testing folders\n",
    "for img in vehicle_images[num_train_vehicles:]:\n",
    "    shutil.copy(src_vehicles_path + img, dest_test_vehicles_path + img)\n",
    "\n",
    "for img in non_vehicle_images[num_train_non_vehicles:]:\n",
    "    shutil.copy(src_non_vehicles_path + img, dest_test_non_vehicles_path + img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.metrics import Precision, Recall, F1Score\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.applications import  MobileNetV2\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import seaborn as sns\n",
    "from tensorflow.keras import models, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3553 images belonging to 2 classes.\n",
      "Found 14207 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "Batch_size1 = 32\n",
    "# Define data augmentation parameters for training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    brightness_range=[0.5, 1.5], \n",
    "    channel_shift_range=50 #shifts the color channels \n",
    ")\n",
    "# Create a data generator for test data\n",
    "test_datagen = ImageDataGenerator(rescale=1./255\n",
    ")\n",
    "\n",
    "test_Image = test_datagen.flow_from_directory(\n",
    "    './test',\n",
    "    target_size=(1500, 1000),\n",
    "    batch_size=Batch_size1,\n",
    "    class_mode='binary',\n",
    "    shuffle=False  # Important: Set shuffle to False for evaluation\n",
    ")\n",
    "\n",
    "train_Image = train_datagen.flow_from_directory(\n",
    "    './train',\n",
    "    target_size=(1500, 1000),\n",
    "    batch_size=Batch_size1,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D,UpSampling2D, Conv2DTranspose\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2 = Sequential() # initialize neural network\n",
    "        # Convolutional layers\n",
    "classifier2.add(Conv2D(3, kernel_size=(3,3),strides=(1, 1), input_shape=(1500, 1000, 3), activation='PReLU',padding='same'))\n",
    "        #classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "classifier2.add(Conv2D(35, kernel_size=(3,3),strides=(1, 1), activation='PReLU',padding='same'))\n",
    "        #classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "classifier2.add(Conv2D(35, kernel_size=(3,3),strides=(1, 1), activation='PReLU',padding='same'))\n",
    "classifier2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "classifier2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "classifier2.add(UpSampling2D(size=(2, 2)))\n",
    "classifier2.add(Conv2DTranspose(64, (3, 3), strides=(1, 1), padding='same',activation='PReLU'))\n",
    "\n",
    "# Add PReLU activation\n",
    "        # classifier.add(PReLU())\n",
    "        \n",
    "# Add another Deconvolutional (Conv2DTranspose) layer to upsample the feature maps\n",
    "classifier2.add(Conv2DTranspose(32, (3, 3), strides=(1, 1), padding='same',activation='PReLU'))\n",
    "\n",
    "# Add PReLU activation\n",
    "        # model.add(PReLU())\n",
    "\n",
    "# Add another Deconvolutional (Conv2DTranspose) layer to upsample the feature maps\n",
    "classifier2.add(Conv2DTranspose(3, (3, 3), strides=(2, 2), padding='same', activation='sigmoid'))\n",
    "lr = 0.003\n",
    "adam0 = Adam(learning_rate=lr)\n",
    "classifier2.compile(optimizer=adam0, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py\", line 2122, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend.py\", line 5560, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None,) and (None, 1500, 1000, 3) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Evaluate1 \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_Image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_Image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBatch_size1\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filev13wrusb.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py\", line 2122, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend.py\", line 5560, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None,) and (None, 1500, 1000, 3) are incompatible\n"
     ]
    }
   ],
   "source": [
    "Evaluate1 = classifier2.fit(train_Image, epochs=10, validation_data=test_Image, batch_size=Batch_size1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D,UpSampling2D, Conv2DTranspose\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_image_array_2 = input_image_array.reshape(1, rows, cols, channels)\n",
    "print(input_image_array_2)\n",
    "def build_classifier(input_shape):\n",
    "    def bm():\n",
    "        classifier = Sequential() # initialize neural network\n",
    "        # Convolutional layers\n",
    "        classifier.add(Conv2D(3, kernel_size=(3,3),strides=(1, 1), input_shape=input_shape, activation='PReLU',padding='same'))\n",
    "        #classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        classifier.add(Conv2D(35, kernel_size=(3,3),strides=(1, 1), activation='PReLU',padding='same'))\n",
    "        #classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        classifier.add(Conv2D(35, kernel_size=(3,3),strides=(1, 1), activation='PReLU',padding='same'))\n",
    "        classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        classifier.add(UpSampling2D(size=(2, 2)))\n",
    "        classifier.add(Conv2DTranspose(64, (3, 3), strides=(1, 1), padding='same',activation='PReLU'))\n",
    "\n",
    "# Add PReLU activation\n",
    "        # classifier.add(PReLU())\n",
    "        \n",
    "# Add another Deconvolutional (Conv2DTranspose) layer to upsample the feature maps\n",
    "        classifier.add(Conv2DTranspose(32, (3, 3), strides=(1, 1), padding='same',activation='PReLU'))\n",
    "\n",
    "# Add PReLU activation\n",
    "        # model.add(PReLU())\n",
    "\n",
    "# Add another Deconvolutional (Conv2DTranspose) layer to upsample the feature maps\n",
    "        classifier.add(Conv2DTranspose(3, (3, 3), strides=(2, 2), padding='same', activation='sigmoid'))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # # Flatten layer\n",
    "        # classifier.add(Flatten())\n",
    "        \n",
    "        # Fully connected layers\n",
    "        #classifier.add(Dense(units=80, activation='relu'))\n",
    "        #classifier.add(Dense(units=448, activation='relu'))\n",
    "    \n",
    "    \n",
    "        # Compile the model\n",
    "        lr = 0.003\n",
    "        adam0 = Adam(learning_rate=lr)\n",
    "        classifier.compile(optimizer=adam0, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "\n",
    "# Predict the output\n",
    "        output_image =classifier.predict(input_image_array_2)\n",
    "        print(output_image.shape[3])\n",
    "# Convert output to image\n",
    "        output_image = array_to_img(output_image[0])\n",
    "\n",
    "# Save the output image\n",
    "        output_image.save(\"converted.jpg\")\n",
    "        return classifier\n",
    "    return bm\n",
    "\n",
    "# Usage example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (rows, cols, channels)\n",
    "classifier = build_classifier(input_shape)()\n",
    "print(\"a\",classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
